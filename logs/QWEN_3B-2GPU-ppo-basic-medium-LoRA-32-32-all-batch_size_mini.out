INFO 06-17 12:52:57 [__init__.py:239] Automatically detected platform cuda.
config: {'data': {'tokenizer': None, 'train_files': '~/data/rlhf/gsm8k/train.parquet', 'val_files': '~/data/rlhf/gsm8k/test.parquet', 'prompt_key': 'prompt', 'reward_fn_key': 'data_source', 'max_prompt_length': None, 'max_response_length': None, 'train_batch_size': 128, 'val_batch_size': None, 'return_raw_input_ids': False, 'return_raw_chat': False, 'shuffle': True, 'filter_overlong_prompts': False, 'filter_overlong_prompts_workers': 1, 'truncation': 'error', 'image_key': 'images', 'video_key': 'videos', 'custom_cls': {'path': None, 'name': None}}, 'actor_rollout_ref': {'hybrid_engine': True, 'model': {'path': '${model_path}', 'external_lib': None, 'override_config': {}, 'enable_gradient_checkpointing': True, 'use_remove_padding': False, 'use_liger': False, 'lora_rank': '${lora.rank}', 'lora_alpha': '${lora.alpha}', 'target_modules': '${lora.target_modules}'}, 'actor': {'strategy': 'fsdp', 'ppo_mini_batch_size': '${ppo_mini_batch_size}', 'ppo_micro_batch_size': None, 'ppo_micro_batch_size_per_gpu': '${micro_batch_size_per_gpu}', 'use_dynamic_bsz': False, 'ppo_max_token_len_per_gpu': 16384, 'grad_clip': 1.0, 'clip_ratio': 0.2, 'clip_ratio_low': 0.2, 'clip_ratio_high': 0.2, 'clip_ratio_c': 3.0, 'loss_agg_mode': 'token-mean', 'entropy_coeff': 0.001, 'use_kl_loss': False, 'use_torch_compile': True, 'kl_loss_coef': 0.001, 'kl_loss_type': 'kl', 'ppo_epochs': 1, 'shuffle': False, 'ulysses_sequence_parallel_size': 1, 'checkpoint': {'contents': ['model', 'optimizer', 'extra']}, 'optim': {'lr': 1e-06, 'lr_warmup_steps': -1, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': -1, 'weight_decay': 0.01, 'betas': [0.9, 0.999]}, 'fsdp_config': {'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'optimizer_offload': False, 'offload_policy': False, 'reshard_after_forward': True, 'fsdp_size': -1}, 'micro_batch_size_per_gpu': '${micro_batch_size_per_gpu}', 'use_ref': True, 'grpo_advantage_length_weight': '${grpo_advantage_length_weight}'}, 'ref': {'strategy': 'fsdp', 'fsdp_config': {'param_offload': False, 'reshard_after_forward': True, 'wrap_policy': {'min_num_params': 0}}, 'use_torch_compile': '${actor_rollout_ref.actor.use_torch_compile}', 'log_prob_micro_batch_size': None, 'log_prob_micro_batch_size_per_gpu': '${micro_batch_size_per_gpu}', 'log_prob_use_dynamic_bsz': '${actor_rollout_ref.actor.use_dynamic_bsz}', 'log_prob_max_token_len_per_gpu': '${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}', 'ulysses_sequence_parallel_size': '${actor_rollout_ref.actor.ulysses_sequence_parallel_size}'}, 'rollout': {'name': 'vllm', 'mode': 'sync', 'chat_scheduler': None, 'temperature': 1, 'top_k': -1, 'top_p': 1, 'use_fire_sampling': False, 'prompt_length': 1, 'response_length': 256, 'dtype': 'bfloat16', 'gpu_memory_utilization': 0.75, 'ignore_eos': False, 'enforce_eager': True, 'free_cache_engine': True, 'load_format': 'dummy_dtensor', 'tensor_model_parallel_size': 2, 'max_num_batched_tokens': 8192, 'max_model_len': 4096, 'max_num_seqs': 1024, 'log_prob_micro_batch_size': None, 'log_prob_micro_batch_size_per_gpu': '${micro_batch_size_per_gpu}', 'log_prob_use_dynamic_bsz': '${actor_rollout_ref.actor.use_dynamic_bsz}', 'log_prob_max_token_len_per_gpu': '${actor_rollout_ref.actor.ppo_max_token_len_per_gpu}', 'disable_log_stats': True, 'enable_chunked_prefill': True, 'do_sample': True, 'n': 1, 'engine_kwargs': {'swap_space': 200}, 'val_kwargs': {'top_k': -1, 'top_p': 1.0, 'temperature': 0.5, 'n': 1, 'do_sample': True}, 'multi_turn': {'enable': False, 'max_turns': None, 'tool_config_path': None, 'format': 'chatml'}, 'rollout_filter_ratio': 1, 'rollout_filter_type': 'std', 'tp_size_check': True}}, 'critic': {'rollout_n': '${actor_rollout_ref.rollout.n}', 'strategy': 'fsdp', 'optim': {'lr': 1e-05, 'lr_warmup_steps_ratio': 0.0, 'min_lr_ratio': None, 'warmup_style': 'constant', 'total_training_steps': -1, 'weight_decay': 0.01, 'betas': [0.9, 0.999]}, 'model': {'path': '${model_path}', 'tokenizer_path': '${actor_rollout_ref.model.path}', 'override_config': {}, 'external_lib': '${actor_rollout_ref.model.external_lib}', 'enable_gradient_checkpointing': True, 'use_remove_padding': False, 'fsdp_config': {'param_offload': False, 'optimizer_offload': False, 'offload_policy': False, 'reshard_after_forward': True, 'wrap_policy': {'min_num_params': 0}, 'fsdp_size': -1}, 'lora_rank': '${lora.rank}', 'lora_alpha': '${lora.alpha}', 'target_modules': '${lora.target_modules}'}, 'ppo_mini_batch_size': '${ppo_mini_batch_size}', 'ppo_micro_batch_size': None, 'ppo_micro_batch_size_per_gpu': '${micro_batch_size_per_gpu}', 'forward_micro_batch_size': '${critic.ppo_micro_batch_size}', 'forward_micro_batch_size_per_gpu': '${critic.ppo_micro_batch_size_per_gpu}', 'use_dynamic_bsz': '${actor_rollout_ref.actor.use_dynamic_bsz}', 'ppo_max_token_len_per_gpu': 32768, 'forward_max_token_len_per_gpu': '${critic.ppo_max_token_len_per_gpu}', 'ulysses_sequence_parallel_size': 1, 'ppo_epochs': '${actor_rollout_ref.actor.ppo_epochs}', 'shuffle': '${actor_rollout_ref.actor.shuffle}', 'grad_clip': 1.0, 'cliprange_value': 0.5, 'checkpoint': {'contents': ['model', 'optimizer', 'extra']}}, 'reward_model': {'enable': False, 'strategy': 'fsdp', 'model': {'input_tokenizer': '${actor_rollout_ref.model.path}', 'path': '~/models/FsfairX-LLaMA3-RM-v0.1', 'external_lib': '${actor_rollout_ref.model.external_lib}', 'use_remove_padding': False, 'fsdp_config': {'wrap_policy': {'min_num_params': 0}, 'param_offload': False, 'reshard_after_forward': True, 'fsdp_size': -1}}, 'micro_batch_size': None, 'micro_batch_size_per_gpu': None, 'max_length': None, 'ulysses_sequence_parallel_size': 1, 'use_dynamic_bsz': '${critic.use_dynamic_bsz}', 'forward_max_token_len_per_gpu': '${critic.forward_max_token_len_per_gpu}', 'reward_manager': 'naive', 'launch_reward_fn_async': False}, 'custom_reward_function': {'path': None, 'name': 'compute_score'}, 'algorithm': {'gamma': 1.0, 'lam': 1.0, 'adv_estimator': 'gae', 'norm_adv_by_std_in_grpo': True, 'use_kl_in_reward': False, 'kl_penalty': 'kl', 'kl_ctrl': {'type': 'fixed', 'kl_coef': 0.001, 'horizon': 10000, 'target_kl': 0.1}, 'high_level_gamma': 0.95, 'bi_level_gae': False}, 'trainer': {'balance_batch': True, 'total_epochs': 30, 'total_training_steps': 200, 'project_name': 'frozen_lake', 'experiment_name': 'QWEN_3B-2GPU-ppo-basic-medium-LoRA-32-32-all-batch_size_mini', 'logger': ['console', 'wandb'], 'log_val_generations': 0, 'rollout_data_dir': None, 'validation_data_dir': None, 'nnodes': 1, 'n_gpus_per_node': 2, 'save_freq': -1, 'resume_mode': 'auto', 'resume_from_path': None, 'val_before_train': True, 'test_freq': 10, 'critic_warmup': 0, 'default_hdfs_dir': None, 'del_local_ckpt_after_load': False, 'default_local_dir': 'checkpoints/${trainer.project_name}/${trainer.experiment_name}', 'max_actor_ckpt_to_keep': None, 'max_critic_ckpt_to_keep': None, 'ray_wait_register_center_timeout': 300, 'validation_steps': 1, 'generations_to_log_to_wandb': {'train': 128, 'val': 20}}, 'ray_init': {'num_cpus': None}, 'custom_envs': {'SimpleSokoban': {'env_type': 'sokoban', 'max_actions_per_traj': 10, 'env_instruction': 'You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer should be a sequence of actions, like <answer>Right || Right || Up</answer>', 'max_tokens': 100, 'env_config': {'dim_x': 6, 'dim_y': 6, 'num_boxes': 1, 'max_steps': 100}}, 'LargerSokoban': {'env_type': 'sokoban', 'max_actions_per_traj': 10, 'env_instruction': 'You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer should be a sequence of actions, like <answer>Right || Right || Up</answer>', 'max_tokens': 100, 'env_config': {'dim_x': 8, 'dim_y': 8, 'num_boxes': 2, 'max_steps': 100, 'search_depth': 10}}, 'SokobanDifferentGridVocab': {'env_type': 'sokoban', 'max_actions_per_traj': 10, 'env_instruction': 'You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer should be a sequence of actions, like <answer>Right || Right || Up</answer>', 'max_tokens': 100, 'env_config': {'search_depth': 30, 'dim_x': 6, 'dim_y': 6, 'num_boxes': 1, 'max_steps': 100, 'grid_lookup': {0: 'W', 1: '.', 2: 'G', 3: 'C', 4: 'B', 5: 'A', 6: '@'}, 'grid_vocab': {'W': 'wall', '.': 'empty', 'G': 'target', 'C': 'box on target', 'B': 'box', 'A': 'player', '@': 'player on target'}}}, 'VisualSimpleSokoban': {'env_type': 'sokoban', 'max_actions_per_traj': 10, 'env_instruction': 'You are solving the Sokoban puzzle. You are the player and you need to push all boxes to targets. When you are right next to a box, you can push it by moving in the same direction. You cannot push a box through a wall, and you cannot pull a box. The answer should be a sequence of actions, like <answer>Right || Right || Up</answer>', 'max_tokens': 100, 'env_config': {'dim_x': 6, 'dim_y': 6, 'num_boxes': 1, 'max_steps': 100, 'render_mode': 'rgb_array'}}, 'Countdown': {'env_type': 'countdown', 'max_actions_per_traj': 1, 'env_instruction': "You are solving the Countdown puzzle. You should use the num list to create an equation that equals the target. Example answer format: <think> To find an equation using [3, 5, 2] to get 4. Let's check 2 + 5 = 7, 7 - 3 = 4. So the answer is 2 + 5 - 3 = 4. </think><answer>2 + 5 - 3</answer>", 'max_tokens': 100, 'env_config': None}, 'Bandit': {'env_type': 'bandit', 'max_actions_per_traj': 1, 'env_instruction': '', 'max_tokens': 100, 'env_config': {'lo_arm_name': 'Phoenix', 'hi_arm_name': 'Dragon'}}, 'BanditTest': {'env_type': 'bandit', 'max_actions_per_traj': 1, 'env_instruction': '', 'max_tokens': 100, 'env_config': {'lo_arm_name': 'Trader', 'hi_arm_name': 'Librarian'}}, 'FrozenLake': {'env_type': 'frozen_lake', 'max_actions_per_traj': 10, 'env_instruction': 'You are solving the FrozenLake puzzle. Forbid the whole and go to the target. You may move to the unintended direction due to the slippery ice. Example answer format: <think>To forbid the hole and go to the target, I should go left then go up.</think><answer>Left || Up</answer>', 'max_tokens': 100, 'env_config': None}, 'MetamathQA': {'env_type': 'metamathqa', 'max_actions_per_traj': 1, 'env_instruction': 'You are solving Math problems. ', 'max_tokens': 100, 'env_config': None}, 'WebShop': {'env_type': 'webshop', 'max_actions_per_traj': 9, 'env_instruction': 'You are browsing an online shop. Based on the instruction, buy a product that close to the production description. You need to search, read the search results, pick a product, choose the size and color and buy. You should only choose action from the available actions list provided later.  Example process: I need a gingko light and 20x20 pillow cover that is hand painted. First search[gingko light 20x20 pillow cover hand painted], answer format: <answer>search[blanket with fleece throw]</answer>. Valid answer is search[<keywords>] or click[<clickable>].', 'max_tokens': 200, 'env_config': {'dataset': 'small'}}}, 'system': {'CUDA_VISIBLE_DEVICES': '2,3'}, 'micro_batch_size_per_gpu': 1, 'ppo_mini_batch_size': 4, 'model_path': 'Qwen/Qwen2.5-3B-Instruct', 'enable_response_mask': True, 'grpo_advantage_length_weight': False, 'lora': {'rank': 32, 'alpha': 32, 'target_modules': 'all-linear'}, 'agent_proxy': {'max_turn': 5, 'action_sep': '||', 'max_actions_per_turn': 5, 'use_turn_scores': False, 'enable_think': True, 'reward_normalization': {'grouping': 'state', 'method': 'identity'}}, 'es_manager': {'format_penalty': -0.1, 'train': {'env_groups': 8, 'group_size': 16, 'env_configs': {'tags': ['FrozenLake'], 'n_groups': [8]}}, 'val': {'env_groups': 256, 'group_size': 1, 'env_configs': {'tags': ['FrozenLake'], 'n_groups': [256]}}}, 'ctx_manager': {'generation': {'gen_config': {'response_length': '${actor_rollout_ref.rollout.response_length}', 'temperature': '${actor_rollout_ref.rollout.temperature}', 'top_p': '${actor_rollout_ref.rollout.top_p}', 'top_k': '${actor_rollout_ref.rollout.top_k}', 'kwargs': None}}}}
CUDA_VISIBLE_DEVICES: 2,3
[36m(TaskRunner pid=3616073)[0m [DEBUG] using ref policy
[36m(TaskRunner pid=3616073)[0m using dummy reward manager
[36m(TaskRunner pid=3616073)[0m [validate_config] All configuration checks passed successfully!
[36m(TaskRunner pid=3616073)[0m Total training steps: 200
[36m(TaskRunner pid=3616073)[0m colocated worker base class <class 'verl.single_controller.base.worker.Worker'>
[36m(WorkerDict pid=3616641)[0m Critic overriding config {'bos_token_id': None, 'eos_token_id': 151645, 'pad_token_id': 151643}
[36m(WorkerDict pid=3616900)[0m Applying LoRA to critic module
[36m(WorkerDict pid=3616641)[0m PeftModelForCausalLM contains 3.15B parameters
[36m(WorkerDict pid=3616641)[0m Before critic FSDP, memory allocated (GB): 0.00, memory reserved (GB): 0.00, device memory used/total (GB): 0.42/44.52
[36m(WorkerDict pid=3616641)[0m NCCL version 2.21.5+cuda12.4
[36m(WorkerDict pid=3616641)[0m After critic FSDP, memory allocated (GB): 5.89, memory reserved (GB): 10.24, device memory used/total (GB): 10.92/44.52
[36m(WorkerDict pid=3616641)[0m Total steps: 200, num_warmup_steps: 0
[36m(WorkerDict pid=3616641)[0m Critic use_remove_padding=False
[36m(WorkerDict pid=3616641)[0m Model config after override: Qwen2Config {
[36m(WorkerDict pid=3616641)[0m   "architectures": [
[36m(WorkerDict pid=3616641)[0m     "Qwen2ForCausalLM"
[36m(WorkerDict pid=3616641)[0m   ],
[36m(WorkerDict pid=3616641)[0m   "attention_dropout": 0.0,
[36m(WorkerDict pid=3616641)[0m   "eos_token_id": 151645,
[36m(WorkerDict pid=3616641)[0m   "hidden_act": "silu",
[36m(WorkerDict pid=3616641)[0m   "hidden_size": 2048,
[36m(WorkerDict pid=3616641)[0m   "initializer_range": 0.02,
[36m(WorkerDict pid=3616641)[0m   "intermediate_size": 11008,
[36m(WorkerDict pid=3616641)[0m   "max_position_embeddings": 32768,
[36m(WorkerDict pid=3616641)[0m   "max_window_layers": 70,
[36m(WorkerDict pid=3616641)[0m   "model_type": "qwen2",
[36m(WorkerDict pid=3616641)[0m   "num_attention_heads": 16,
[36m(WorkerDict pid=3616641)[0m   "num_hidden_layers": 36,
[36m(WorkerDict pid=3616641)[0m   "num_key_value_heads": 2,
[36m(WorkerDict pid=3616641)[0m   "pad_token_id": 151643,
[36m(WorkerDict pid=3616641)[0m   "rms_norm_eps": 1e-06,
[36m(WorkerDict pid=3616641)[0m   "rope_scaling": null,
[36m(WorkerDict pid=3616641)[0m   "rope_theta": 1000000.0,
[36m(WorkerDict pid=3616641)[0m   "sliding_window": 32768,
[36m(WorkerDict pid=3616641)[0m   "tie_word_embeddings": true,
[36m(WorkerDict pid=3616641)[0m   "torch_dtype": "bfloat16",
[36m(WorkerDict pid=3616641)[0m   "transformers_version": "4.52.3",
[36m(WorkerDict pid=3616641)[0m   "use_cache": true,
[36m(WorkerDict pid=3616641)[0m   "use_sliding_window": false,
[36m(WorkerDict pid=3616641)[0m   "vocab_size": 151936
[36m(WorkerDict pid=3616641)[0m }
[36m(WorkerDict pid=3616641)[0m 
[36m(WorkerDict pid=3616900)[0m Applying LoRA to actor module
[36m(WorkerDict pid=3616641)[0m Applying LoRA to critic module
[36m(WorkerDict pid=3616900)[0m Total steps: 200, num_warmup_steps: 0
[36m(WorkerDict pid=3616900)[0m Critic use_remove_padding=False
[36m(WorkerDict pid=3616641)[0m PeftModelForCausalLM contains 3.15B parameters
[36m(WorkerDict pid=3616641)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f9d551b6ca0>, policies=[functools.partial(<function lambda_auto_wrap_policy at 0x7f9d551b6660>, lambda_fn=<function get_fsdp_wrap_policy.<locals>.lambda_policy_fn at 0x7f9d091fe160>), functools.partial(<function transformer_auto_wrap_policy at 0x7f9d551b6b60>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3616641)[0m Total steps: 200, num_warmup_steps: 0
[36m(WorkerDict pid=3616641)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=3616641)[0m WARNING 06-17 12:53:42 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=3616641)[0m Applying LoRA to actor module
[36m(WorkerDict pid=3616900)[0m wrap_policy: functools.partial(<function _or_policy at 0x7f50356b2ca0>, policies=[functools.partial(<function lambda_auto_wrap_policy at 0x7f50356b2660>, lambda_fn=<function get_fsdp_wrap_policy.<locals>.lambda_policy_fn at 0x7f4fa84f58a0>), functools.partial(<function transformer_auto_wrap_policy at 0x7f50356b2b60>, transformer_layer_cls={<class 'transformers.models.qwen2.modeling_qwen2.Qwen2DecoderLayer'>})])
[36m(WorkerDict pid=3616900)[0m Total steps: 200, num_warmup_steps: 0
[36m(WorkerDict pid=3616900)[0m Actor use_remove_padding=False
[36m(WorkerDict pid=3616641)[0m WARNING 06-17 12:53:43 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f9c0ddd4200>
[36m(WorkerDict pid=3616900)[0m WARNING 06-17 12:53:44 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[36m(WorkerDict pid=3616641)[0m kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 256, 'detokenize': False, 'temperature': 1, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(WorkerDict pid=3616900)[0m WARNING 06-17 12:53:43 [cuda.py:95] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used
[36m(WorkerDict pid=3616900)[0m WARNING 06-17 12:53:43 [utils.py:2321] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7f4effbadb50>
[36m(TaskRunner pid=3616073)[0m Using LocalLogger is deprecated. The constructor API will change 
[36m(TaskRunner pid=3616073)[0m Checkpoint tracker file does not exist: %s /scr/jwiseman/RAGEN/checkpoints/frozen_lake/QWEN_3B-2GPU-ppo-basic-medium-LoRA-32-32-all-batch_size_mini/latest_checkpointed_iteration.txt
[36m(TaskRunner pid=3616073)[0m Training from scratch
[36m(TaskRunner pid=3616073)[0m test_gen_batch meta info: {'eos_token_id': 151645, 'pad_token_id': 151643, 'recompute_log_prob': False, 'do_sample': True, 'validate': True}
[36m(WorkerDict pid=3616641)[0m WARNING 06-17 12:53:44 [topk_topp_sampler.py:63] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[36m(TaskRunner pid=3616073)[0m validation generation time: 74.82801365852356 seconds
[36m(WorkerDict pid=3616900)[0m kwargs: {'n': 1, 'logprobs': 0, 'max_tokens': 256, 'detokenize': False, 'temperature': 1, 'top_k': -1, 'top_p': 1, 'ignore_eos': False}
[36m(TaskRunner pid=3616073)[0m ("Initial validation metrics: {'val-env/FrozenLake/success': 0.12890625, "
[36m(TaskRunner pid=3616073)[0m  "'val-env/FrozenLake/num_actions': 4.43359375, "
[36m(TaskRunner pid=3616073)[0m  "'val-env/FrozenLake/action_is_effective': 0.8168619791666666, "
[36m(TaskRunner pid=3616073)[0m  "'val-env/FrozenLake/action_is_valid': 0.99375, "
[36m(TaskRunner pid=3616073)[0m  "'val-env/FrozenLake/non-zero/success': 1.0, "
[36m(TaskRunner pid=3616073)[0m  "'val-env/FrozenLake/non-zero/num_actions': 4.43359375, "
[36m(TaskRunner pid=3616073)[0m  "'val-env/FrozenLake/non-zero/action_is_effective': 0.8168619791666666, "
[36m(TaskRunner pid=3616073)[0m  "'val-env/FrozenLake/non-zero/action_is_valid': 0.99375, "
[36m(TaskRunner pid=3616073)[0m  "'val-env/response_length': 154.4765625, 'val-core/unknown/reward/mean@256': "
[36m(TaskRunner pid=3616073)[0m  "0.12578124992433004, 'val-aux/unknown/reward/std@256': 0.33720961908516484, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@2/mean': 0.228, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@2/std': 0.41954260808647315, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@2/mean': 0.0052999997958540915, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@2/std': 0.12075558001098695, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@4/mean': 0.422, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@4/std': 0.4938785275753544, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@4/mean': -0.013600000396370888, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@4/std': 0.06288911026629626, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@8/mean': 0.68, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@8/std': 0.466476151587624, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@8/mean': -0.02930000078678131, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@8/std': 0.07356296861099039, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@16/mean': 0.889, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@16/std': 0.31413213780191296, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@16/mean': -0.05100000131130218, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@16/std': 0.08954887220704474, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@32/mean': 0.987, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@32/std': 0.11327400407860576, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@32/mean': -0.0852000022828579, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@32/std': 0.10630597712311153, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@64/mean': 0.999, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@64/std': 0.03160696125855822, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@64/mean': -0.131400003567338, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@64/std': 0.10979089626149763, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@128/mean': 1.0, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/best@128/std': 0.0, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@128/mean': -0.196500005915761, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@128/std': 0.09837556051616578, "
[36m(TaskRunner pid=3616073)[0m  "'val-core/unknown/reward/best@256/mean': 1.0, "
[36m(TaskRunner pid=3616073)[0m  "'val-aux/unknown/reward/worst@256/mean': -0.30000001192092896}")
[36m(TaskRunner pid=3616073)[0m step:0 - val-env/FrozenLake/success:0.129 - val-env/FrozenLake/num_actions:4.434 - val-env/FrozenLake/action_is_effective:0.817 - val-env/FrozenLake/action_is_valid:0.994 - val-env/FrozenLake/non-zero/success:1.000 - val-env/FrozenLake/non-zero/num_actions:4.434 - val-env/FrozenLake/non-zero/action_is_effective:0.817 - val-env/FrozenLake/non-zero/action_is_valid:0.994 - val-env/response_length:154.477 - val-core/unknown/reward/mean@256:0.126 - val-aux/unknown/reward/std@256:0.337 - val-aux/unknown/reward/best@2/mean:0.228 - val-aux/unknown/reward/best@2/std:0.420 - val-aux/unknown/reward/worst@2/mean:0.005 - val-aux/unknown/reward/worst@2/std:0.121 - val-aux/unknown/reward/best@4/mean:0.422 - val-aux/unknown/reward/best@4/std:0.494 - val-aux/unknown/reward/worst@4/mean:-0.014 - val-aux/unknown/reward/worst@4/std:0.063 - val-aux/unknown/reward/best@8/mean:0.680 - val-aux/unknown/reward/best@8/std:0.466 - val-aux/unknown/reward/worst@8/mean:-0.029 - val-aux/unknown/reward/worst@8/std:0.074 - val-aux/unknown/reward/best@16/mean:0.889 - val-aux/unknown/reward/best@16/std:0.314 - val-aux/unknown/reward/worst@16/mean:-0.051 - val-aux/unknown/reward/worst@16/std:0.090 - val-aux/unknown/reward/best@32/mean:0.987 - val-aux/unknown/reward/best@32/std:0.113 - val-aux/unknown/reward/worst@32/mean:-0.085 - val-aux/unknown/reward/worst@32/std:0.106 - val-aux/unknown/reward/best@64/mean:0.999 - val-aux/unknown/reward/best@64/std:0.032 - val-aux/unknown/reward/worst@64/mean:-0.131 - val-aux/unknown/reward/worst@64/std:0.110 - val-aux/unknown/reward/best@128/mean:1.000 - val-aux/unknown/reward/best@128/std:0.000 - val-aux/unknown/reward/worst@128/mean:-0.197 - val-aux/unknown/reward/worst@128/std:0.098 - val-core/unknown/reward/best@256/mean:1.000 - val-aux/unknown/reward/worst@256/mean:-0.300
[36m(TaskRunner pid=3616073)[0m Error in reward_fn: 'reward_extra_info'
[36m(WorkerDict pid=3616641)[0m [INFO] Actor is a PeftModel
[36m(WorkerDict pid=3616641)[0m [INFO] Merged adapter actor
[36m(WorkerDict pid=3616641)[0m [INFO] Unmerging adapter actor
[36m(WorkerDict pid=3616900)[0m [INFO] Actor is a PeftModel
[36m(WorkerDict pid=3616900)[0m [INFO] Merged adapter actor
[36m(WorkerDict pid=3616641)[0m [INFO] Unmerged adapter actor
[36m(WorkerDict pid=3616641)[0m [INFO] Critic is a PeftModel
[36m(WorkerDict pid=3616900)[0m [INFO] Unmerging adapter actor
[36m(WorkerDict pid=3616900)[0m [INFO] Unmerged adapter actor
[36m(WorkerDict pid=3616641)[0m [INFO] Merged adapter critic
[36m(WorkerDict pid=3616641)[0m [INFO] Unmerging adapter critic
[36m(WorkerDict pid=3616900)[0m [INFO] Critic is a PeftModel
[36m(WorkerDict pid=3616900)[0m [INFO] Merged adapter critic
[36m(WorkerDict pid=3616641)[0m [INFO] Unmerged adapter critic
[36m(WorkerDict pid=3616900)[0m [INFO] Unmerging adapter critic
[36m(WorkerDict pid=3616900)[0m [INFO] Unmerged adapter critic
