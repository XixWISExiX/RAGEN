/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In '_3_frozen_lake': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
2025-06-13 18:06:03,282	INFO worker.py:1888 -- Started a local Ray instance.
[36m(TaskRunner pid=974554)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=974554)[0m WARNING:2025-06-13 18:06:12,695:Waiting for register center actor 53Cvmm_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=975910)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForTokenClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=975910)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=975910)[0m Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.bias', 'score.weight']
[36m(WorkerDict pid=975910)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[36m(WorkerDict pid=975910)[0m /home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=975910)[0m   warnings.warn(
[36m(WorkerDict pid=975910)[0m /home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=975910)[0m   warnings.warn(
[36m(WorkerDict pid=975910)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=975910)[0m /home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=975910)[0m   warnings.warn(
[36m(WorkerDict pid=975910)[0m /home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=975910)[0m   warnings.warn(
[36m(TaskRunner pid=974554)[0m wandb: Currently logged in as: addalooptoit (ounlp) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(TaskRunner pid=974554)[0m wandb: Tracking run with wandb version 0.19.11
[36m(TaskRunner pid=974554)[0m wandb: Run data is saved locally in /scr/jwiseman/RAGEN/wandb/run-20250613_180636-3yvx0gcf
[36m(TaskRunner pid=974554)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(TaskRunner pid=974554)[0m wandb: Syncing run ppo-basic-mini-batch_size_mini-low_clip_symmetric
[36m(TaskRunner pid=974554)[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/ounlp/frozen_lake
[36m(TaskRunner pid=974554)[0m wandb: üöÄ View run at https://wandb.ai/ounlp/frozen_lake/runs/3yvx0gcf
[36m(WorkerDict pid=975910)[0m /home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:773: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.
[36m(WorkerDict pid=975910)[0m   warnings.warn(
[36m(WorkerDict pid=975910)[0m /home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:711: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.
[36m(WorkerDict pid=975910)[0m   warnings.warn(
[36m(TaskRunner pid=974554)[0m Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]
Error executing job with overrides: ['trainer.project_name=frozen_lake', 'system.CUDA_VISIBLE_DEVICES="2"', 'trainer.experiment_name=ppo-basic-mini-batch_size_mini-low_clip_symmetric', 'algorithm.adv_estimator=gae', 'algorithm.kl_ctrl.kl_coef=0.001', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.clip_ratio_high=0.2', 'actor_rollout_ref.rollout.rollout_filter_ratio=1', 'actor_rollout_ref.rollout.max_model_len=1024', 'actor_rollout_ref.rollout.response_length=128', 'micro_batch_size_per_gpu=1', 'ppo_mini_batch_size=4', 'algorithm.kl_ctrl.kl_coef=0.01', 'actor_rollout_ref.actor.kl_loss_coef=0.01', 'actor_rollout_ref.actor.clip_ratio_high=0.1', 'actor_rollout_ref.actor.clip_ratio_low=0.1', 'actor_rollout_ref.rollout.rollout_filter_ratio=1']
Traceback (most recent call last):
  File "/scr/jwiseman/RAGEN/train.py", line 302, in <module>
    main()
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
        ^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/train.py", line 158, in main
    run_ppo(config)
  File "/scr/jwiseman/RAGEN/train.py", line 179, in run_ppo
    ray.get(runner.run.remote(config))
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/worker.py", line 2822, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/worker.py", line 930, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(ValueError): [36mray::TaskRunner.run()[39m (pid=974554, ip=10.255.0.11, actor_id=4c162aabcc8aa6bef78383fc01000000, repr=<train.TaskRunner object at 0x7fe91e915550>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/train.py", line 298, in run
    trainer.fit()
  File "/scr/jwiseman/RAGEN/ragen/trainer/agent_trainer.py", line 514, in fit
    batch = self.agent_proxy.rollout(batch, val=False)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/ragen/llm_agent/agent_proxy.py", line 151, in rollout
    lm_outputs: DataProto = self.generate_sequences(lm_inputs)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/ragen/llm_agent/agent_proxy.py", line 132, in generate_sequences
    padded_lm_outputs = self.actor_wg.generate_sequences(padded_lm_inputs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 49, in func
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(ValueError): [36mray::WorkerDict.actor_rollout_generate_sequences()[39m (pid=975910, ip=10.255.0.11, actor_id=4d041e1b72c87771b3a0da9501000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fb11440b5f0>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 459, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/single_controller/base/decorator.py", line 465, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 614, in generate_sequences
    output = self.rollout.generate_sequences(prompts=prompts)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/utils/debug/performance.py", line 78, in f
    return self.log(decorated_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/utils/debug/performance.py", line 88, in log
    output = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py", line 256, in generate_sequences
    outputs = self.inference_engine.generate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/vllm/utils.py", line 1072, in inner
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 457, in generate
    self._validate_and_add_requests(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 1308, in _validate_and_add_requests
    self._add_request(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/vllm/entrypoints/llm.py", line 1326, in _add_request
    self.llm_engine.add_request(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py", line 184, in add_request
    request = self.processor.process_inputs(request_id, prompt, params,
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/vllm/v1/engine/processor.py", line 207, in process_inputs
    self._validate_model_inputs(processed_inputs, lora_request)
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/vllm/v1/engine/processor.py", line 322, in _validate_model_inputs
    raise ValueError(
ValueError: Prompt length of 1082 is longer than the maximum model length of 1024.
[36m(TaskRunner pid=974554)[0m wandb:                                                                                
[36m(TaskRunner pid=974554)[0m wandb: 
[36m(TaskRunner pid=974554)[0m wandb: Run history:
[36m(TaskRunner pid=974554)[0m wandb:            val-aux/unknown/reward/best@128/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/best@128/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/best@16/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/best@16/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/best@2/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:               val-aux/unknown/reward/best@2/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/best@32/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/best@32/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/best@4/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:               val-aux/unknown/reward/best@4/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/best@64/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/best@64/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/best@8/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:               val-aux/unknown/reward/best@8/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:                  val-aux/unknown/reward/std@256 ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:           val-aux/unknown/reward/worst@128/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:            val-aux/unknown/reward/worst@128/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:            val-aux/unknown/reward/worst@16/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/worst@16/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/worst@2/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/worst@2/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:           val-aux/unknown/reward/worst@256/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:            val-aux/unknown/reward/worst@32/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/worst@32/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/worst@4/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/worst@4/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:            val-aux/unknown/reward/worst@64/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/worst@64/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/worst@8/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/worst@8/std ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:           val-core/unknown/reward/best@256/mean ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:                val-core/unknown/reward/mean@256 ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:          val-env/FrozenLake/action_is_effective ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:              val-env/FrozenLake/action_is_valid ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb: val-env/FrozenLake/non-zero/action_is_effective ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:     val-env/FrozenLake/non-zero/action_is_valid ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:         val-env/FrozenLake/non-zero/num_actions ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:             val-env/FrozenLake/non-zero/success ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:                  val-env/FrozenLake/num_actions ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:                      val-env/FrozenLake/success ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb:                         val-env/response_length ‚ñÅ
[36m(TaskRunner pid=974554)[0m wandb: 
[36m(TaskRunner pid=974554)[0m wandb: Run summary:
[36m(TaskRunner pid=974554)[0m wandb:            val-aux/unknown/reward/best@128/mean 0.9926
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/best@128/std 0.07865
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/best@16/mean 0.448
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/best@16/std 0.52394
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/best@2/mean -0.2483
[36m(TaskRunner pid=974554)[0m wandb:               val-aux/unknown/reward/best@2/std 0.41051
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/best@32/mean 0.7048
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/best@32/std 0.44875
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/best@4/mean -0.0526
[36m(TaskRunner pid=974554)[0m wandb:               val-aux/unknown/reward/best@4/std 0.50271
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/best@64/mean 0.9171
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/best@64/std 0.26343
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/best@8/mean 0.1885
[36m(TaskRunner pid=974554)[0m wandb:               val-aux/unknown/reward/best@8/std 0.54472
[36m(TaskRunner pid=974554)[0m wandb:                  val-aux/unknown/reward/std@256 0.31476
[36m(TaskRunner pid=974554)[0m wandb:           val-aux/unknown/reward/worst@128/mean -0.5
[36m(TaskRunner pid=974554)[0m wandb:            val-aux/unknown/reward/worst@128/std 0
[36m(TaskRunner pid=974554)[0m wandb:            val-aux/unknown/reward/worst@16/mean -0.5
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/worst@16/std 0
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/worst@2/mean -0.4809
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/worst@2/std 0.09168
[36m(TaskRunner pid=974554)[0m wandb:           val-aux/unknown/reward/worst@256/mean -0.5
[36m(TaskRunner pid=974554)[0m wandb:            val-aux/unknown/reward/worst@32/mean -0.5
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/worst@32/std 0
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/worst@4/mean -0.4991
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/worst@4/std 0.00944
[36m(TaskRunner pid=974554)[0m wandb:            val-aux/unknown/reward/worst@64/mean -0.5
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/worst@64/std 0
[36m(TaskRunner pid=974554)[0m wandb:             val-aux/unknown/reward/worst@8/mean -0.5
[36m(TaskRunner pid=974554)[0m wandb:              val-aux/unknown/reward/worst@8/std 0
[36m(TaskRunner pid=974554)[0m wandb:           val-core/unknown/reward/best@256/mean 1
[36m(TaskRunner pid=974554)[0m wandb:                val-core/unknown/reward/mean@256 -0.36953
[36m(TaskRunner pid=974554)[0m wandb:          val-env/FrozenLake/action_is_effective 0.15729
[36m(TaskRunner pid=974554)[0m wandb:              val-env/FrozenLake/action_is_valid 0.18079
[36m(TaskRunner pid=974554)[0m wandb: val-env/FrozenLake/non-zero/action_is_effective 0.64946
[36m(TaskRunner pid=974554)[0m wandb:     val-env/FrozenLake/non-zero/action_is_valid 0.65188
[36m(TaskRunner pid=974554)[0m wandb:         val-env/FrozenLake/non-zero/num_actions 3.46479
[36m(TaskRunner pid=974554)[0m wandb:             val-env/FrozenLake/non-zero/success 1
[36m(TaskRunner pid=974554)[0m wandb:                  val-env/FrozenLake/num_actions 0.96094
[36m(TaskRunner pid=974554)[0m wandb:                      val-env/FrozenLake/success 0.03906
[36m(TaskRunner pid=974554)[0m wandb:                         val-env/response_length 100.21484
[36m(TaskRunner pid=974554)[0m wandb: 
[36m(TaskRunner pid=974554)[0m wandb: üöÄ View run ppo-basic-mini-batch_size_mini-low_clip_symmetric at: https://wandb.ai/ounlp/frozen_lake/runs/3yvx0gcf
[36m(TaskRunner pid=974554)[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/ounlp/frozen_lake
[36m(TaskRunner pid=974554)[0m wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)
[36m(TaskRunner pid=974554)[0m wandb: Find logs at: ./wandb/run-20250613_180636-3yvx0gcf/logs
[36m(TaskRunner pid=974554)[0m Training Progress:   0%|          | 0/200 [00:06<?, ?it/s]
