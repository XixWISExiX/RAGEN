/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In '_3_frozen_lake': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
2025-06-13 15:36:34,702	INFO worker.py:1888 -- Started a local Ray instance.
[36m(TaskRunner pid=528207)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=528207)[0m WARNING:2025-06-13 15:36:43,603:Waiting for register center actor 1okzDS_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=529361)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForTokenClassification is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=529361)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=529361)[0m Some weights of Qwen2ForTokenClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.bias', 'score.weight']
[36m(WorkerDict pid=529361)[0m You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[36m(WorkerDict pid=529361)[0m /home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=529361)[0m   warnings.warn(
[36m(WorkerDict pid=529361)[0m /home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=529361)[0m   warnings.warn(
[36m(WorkerDict pid=529361)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=529361)[0m /home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py:444: UserWarning: FSDP is switching to use `NO_SHARD` instead of ShardingStrategy.FULL_SHARD since the world size is 1.
[36m(WorkerDict pid=529361)[0m   warnings.warn(
[36m(WorkerDict pid=529361)[0m /home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py:690: FutureWarning: FSDP.state_dict_type() and FSDP.set_state_dict_type() are being deprecated. Please use APIs, get_state_dict() and set_state_dict(), which can support different parallelisms, FSDP1, FSDP2, DDP. API doc: https://pytorch.org/docs/stable/distributed.checkpoint.html#torch.distributed.checkpoint.state_dict.get_state_dict .Tutorial: https://pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html .
[36m(WorkerDict pid=529361)[0m   warnings.warn(
[36m(TaskRunner pid=528207)[0m wandb: Currently logged in as: addalooptoit (ounlp) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
[36m(TaskRunner pid=528207)[0m wandb: Tracking run with wandb version 0.19.11
[36m(TaskRunner pid=528207)[0m wandb: Run data is saved locally in /scr/jwiseman/RAGEN/wandb/run-20250613_153706-ugld37lr
[36m(TaskRunner pid=528207)[0m wandb: Run `wandb offline` to turn off syncing.
[36m(TaskRunner pid=528207)[0m wandb: Syncing run ppo-basic-medium-batch_size-8-32
[36m(TaskRunner pid=528207)[0m wandb: ‚≠êÔ∏è View project at https://wandb.ai/ounlp/frozen_lake
[36m(TaskRunner pid=528207)[0m wandb: üöÄ View run at https://wandb.ai/ounlp/frozen_lake/runs/ugld37lr
[36m(WorkerDict pid=529361)[0m /home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:773: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.
[36m(WorkerDict pid=529361)[0m   warnings.warn(
[36m(WorkerDict pid=529361)[0m /home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_state_dict_utils.py:711: UserWarning: When using ``NO_SHARD`` for ``ShardingStrategy``, full_state_dict willbe returned.
[36m(WorkerDict pid=529361)[0m   warnings.warn(
[36m(TaskRunner pid=528207)[0m Training Progress:   0%|          | 0/200 [00:00<?, ?it/s]
Error executing job with overrides: ['trainer.project_name=frozen_lake', 'system.CUDA_VISIBLE_DEVICES="1"', 'trainer.experiment_name=ppo-basic-medium-batch_size-8-32', 'algorithm.adv_estimator=gae', 'algorithm.kl_ctrl.kl_coef=0.001', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.clip_ratio_high=0.2', 'actor_rollout_ref.rollout.rollout_filter_ratio=1', 'actor_rollout_ref.rollout.max_model_len=4096', 'actor_rollout_ref.rollout.response_length=256', 'micro_batch_size_per_gpu=8', 'ppo_mini_batch_size=32']
[36m(TaskRunner pid=528207)[0m wandb:                                                                                
Traceback (most recent call last):
[36m(TaskRunner pid=528207)[0m wandb: 
  File "/scr/jwiseman/RAGEN/train.py", line 302, in <module>
[36m(TaskRunner pid=528207)[0m wandb: Run history:
[36m(TaskRunner pid=528207)[0m wandb:            val-aux/unknown/reward/best@128/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/best@128/std ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/best@16/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/best@16/std ‚ñÅ
    main()
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/best@2/mean ‚ñÅ
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/main.py", line 94, in decorated_main
[36m(TaskRunner pid=528207)[0m wandb:               val-aux/unknown/reward/best@2/std ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/best@32/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/best@32/std ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/best@4/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:               val-aux/unknown/reward/best@4/std ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/best@64/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/best@64/std ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/best@8/mean ‚ñÅ
    _run_hydra(
[36m(TaskRunner pid=528207)[0m wandb:               val-aux/unknown/reward/best@8/std ‚ñÅ
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
[36m(TaskRunner pid=528207)[0m wandb:                  val-aux/unknown/reward/std@256 ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:           val-aux/unknown/reward/worst@128/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:            val-aux/unknown/reward/worst@128/std ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:            val-aux/unknown/reward/worst@16/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/worst@16/std ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/worst@2/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/worst@2/std ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:           val-aux/unknown/reward/worst@256/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:            val-aux/unknown/reward/worst@32/mean ‚ñÅ
    _run_app(
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/worst@32/std ‚ñÅ
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 457, in _run_app
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/worst@4/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/worst@4/std ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:            val-aux/unknown/reward/worst@64/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/worst@64/std ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/worst@8/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/worst@8/std ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:           val-core/unknown/reward/best@256/mean ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:                val-core/unknown/reward/mean@256 ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:          val-env/FrozenLake/action_is_effective ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:              val-env/FrozenLake/action_is_valid ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb: val-env/FrozenLake/non-zero/action_is_effective ‚ñÅ
    run_and_report(
[36m(TaskRunner pid=528207)[0m wandb:     val-env/FrozenLake/non-zero/action_is_valid ‚ñÅ
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
[36m(TaskRunner pid=528207)[0m wandb:         val-env/FrozenLake/non-zero/num_actions ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:             val-env/FrozenLake/non-zero/success ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:                  val-env/FrozenLake/num_actions ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:                      val-env/FrozenLake/success ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb:                         val-env/response_length ‚ñÅ
[36m(TaskRunner pid=528207)[0m wandb: 
    raise ex
[36m(TaskRunner pid=528207)[0m wandb: Run summary:
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
[36m(TaskRunner pid=528207)[0m wandb:            val-aux/unknown/reward/best@128/mean 0.997
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/best@128/std 0.05469
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/best@16/mean 0.5232
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/best@16/std 0.53943
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/best@2/mean -0.2408
[36m(TaskRunner pid=528207)[0m wandb:               val-aux/unknown/reward/best@2/std 0.44029
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/best@32/mean 0.7747
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/best@32/std 0.41734
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/best@4/mean -0.0268
    return func()
[36m(TaskRunner pid=528207)[0m wandb:               val-aux/unknown/reward/best@4/std 0.54275
           ^^^^^^
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/best@64/mean 0.9548
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/best@64/std 0.20346
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/best@8/mean 0.2186
[36m(TaskRunner pid=528207)[0m wandb:               val-aux/unknown/reward/best@8/std 0.58058
[36m(TaskRunner pid=528207)[0m wandb:                  val-aux/unknown/reward/std@256 0.33673
[36m(TaskRunner pid=528207)[0m wandb:           val-aux/unknown/reward/worst@128/mean -0.5
    lambda: hydra.run(
[36m(TaskRunner pid=528207)[0m wandb:            val-aux/unknown/reward/worst@128/std 0
            ^^^^^^^^^^
[36m(TaskRunner pid=528207)[0m wandb:            val-aux/unknown/reward/worst@16/mean -0.5
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/hydra.py", line 132, in run
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/worst@16/std 0
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/worst@2/mean -0.4818
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/worst@2/std 0.09049
[36m(TaskRunner pid=528207)[0m wandb:           val-aux/unknown/reward/worst@256/mean -0.5
[36m(TaskRunner pid=528207)[0m wandb:            val-aux/unknown/reward/worst@32/mean -0.5
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/worst@32/std 0
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/worst@4/mean -0.4992
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/worst@4/std 0.00891
[36m(TaskRunner pid=528207)[0m wandb:            val-aux/unknown/reward/worst@64/mean -0.5
    _ = ret.return_value
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/worst@64/std 0
        ^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=528207)[0m wandb:             val-aux/unknown/reward/worst@8/mean -0.5
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/core/utils.py", line 260, in return_value
[36m(TaskRunner pid=528207)[0m wandb:              val-aux/unknown/reward/worst@8/std 0
[36m(TaskRunner pid=528207)[0m wandb:           val-core/unknown/reward/best@256/mean 1
[36m(TaskRunner pid=528207)[0m wandb:                val-core/unknown/reward/mean@256 -0.36445
[36m(TaskRunner pid=528207)[0m wandb:          val-env/FrozenLake/action_is_effective 0.15241
[36m(TaskRunner pid=528207)[0m wandb:              val-env/FrozenLake/action_is_valid 0.17695
[36m(TaskRunner pid=528207)[0m wandb: val-env/FrozenLake/non-zero/action_is_effective 0.6293
[36m(TaskRunner pid=528207)[0m wandb:     val-env/FrozenLake/non-zero/action_is_valid 0.65652
[36m(TaskRunner pid=528207)[0m wandb:         val-env/FrozenLake/non-zero/num_actions 3.37681
[36m(TaskRunner pid=528207)[0m wandb:             val-env/FrozenLake/non-zero/success 1
[36m(TaskRunner pid=528207)[0m wandb:                  val-env/FrozenLake/num_actions 0.91016
[36m(TaskRunner pid=528207)[0m wandb:                      val-env/FrozenLake/success 0.04688
    raise self._return_value
[36m(TaskRunner pid=528207)[0m wandb:                         val-env/response_length 104
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/core/utils.py", line 186, in run_job
[36m(TaskRunner pid=528207)[0m wandb: 
[36m(TaskRunner pid=528207)[0m wandb: üöÄ View run ppo-basic-medium-batch_size-8-32 at: https://wandb.ai/ounlp/frozen_lake/runs/ugld37lr
[36m(TaskRunner pid=528207)[0m wandb: ‚≠êÔ∏è View project at: https://wandb.ai/ounlp/frozen_lake
[36m(TaskRunner pid=528207)[0m wandb: Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)
[36m(TaskRunner pid=528207)[0m wandb: Find logs at: ./wandb/run-20250613_153706-ugld37lr/logs
[36m(TaskRunner pid=528207)[0m Training Progress:   0%|          | 0/200 [00:35<?, ?it/s]
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/train.py", line 158, in main
    run_ppo(config)
  File "/scr/jwiseman/RAGEN/train.py", line 179, in run_ppo
    ray.get(runner.run.remote(config))
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/worker.py", line 2822, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/worker.py", line 930, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::TaskRunner.run()[39m (pid=528207, ip=10.255.0.11, actor_id=9adbabbaac39d2ae2729dbfa01000000, repr=<train.TaskRunner object at 0x7fe81e345580>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/train.py", line 298, in run
    trainer.fit()
  File "/scr/jwiseman/RAGEN/ragen/trainer/agent_trainer.py", line 654, in fit
    actor_output = self.actor_rollout_wg.update_actor(batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 49, in func
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.actor_rollout_update_actor()[39m (pid=529361, ip=10.255.0.11, actor_id=90fdbb50fbda8b22fef62bc001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fdde552b410>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 459, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/single_controller/base/decorator.py", line 465, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 561, in update_actor
    metrics = self.actor.update_policy(data=data)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/utils/debug/performance.py", line 78, in f
    return self.log(decorated_function, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/utils/debug/performance.py", line 88, in log
    output = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/ragen/workers/actor/dp_actor.py", line 323, in update_policy
    entropy, log_prob = self._forward_micro_batch(micro_batch=data, temperature=temperature, calculate_entropy=calculate_entropy)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/ragen/workers/actor/dp_actor.py", line 156, in _forward_micro_batch
    entropy = verl_F.entropy_from_logits(logits)  # (bsz, response_length)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/utils/torch_functional.py", line 113, in entropy_from_logits
    entropy = torch.logsumexp(logits, dim=-1) - torch.sum(pd * logits, dim=-1)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 8.42 GiB. GPU 0 has a total capacity of 44.52 GiB of which 8.10 GiB is free. Including non-PyTorch memory, this process has 36.41 GiB memory in use. Of the allocated memory 45.72 GiB is allocated by PyTorch, with 78.71 MiB allocated in private pools (e.g., CUDA Graphs), and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
