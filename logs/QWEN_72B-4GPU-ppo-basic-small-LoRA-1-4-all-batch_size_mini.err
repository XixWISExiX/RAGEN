/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In '_3_frozen_lake': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
2025-06-19 10:57:38,050	INFO worker.py:1888 -- Started a local Ray instance.
[36m(TaskRunner pid=726762)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=726762)[0m WARNING:2025-06-19 10:57:47,202:Waiting for register center actor 8lKH6E_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=727296)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=727296)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:   0%|          | 0/37 [00:00<?, ?it/s]
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:   3%|â–Ž         | 1/37 [00:04<02:41,  4.49s/it]
[36m(WorkerDict pid=727505)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=727505)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=727505)[0m Loading checkpoint shards:   0%|          | 0/37 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:   8%|â–Š         | 3/37 [00:13<02:27,  4.35s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:  14%|â–ˆâ–Ž        | 5/37 [00:21<02:19,  4.35s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:  19%|â–ˆâ–‰        | 7/37 [00:29<02:04,  4.13s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:  24%|â–ˆâ–ˆâ–       | 9/37 [00:36<01:45,  3.78s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:  30%|â–ˆâ–ˆâ–‰       | 11/37 [00:43<01:32,  3.57s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=727505)[0m Loading checkpoint shards:  32%|â–ˆâ–ˆâ–ˆâ–      | 12/37 [00:48<01:34,  3.78s/it][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 14/37 [00:54<01:20,  3.51s/it][32m [repeated 5x across cluster][0m
[36m(WorkerDict pid=727504)[0m Loading checkpoint shards:  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 15/37 [00:59<01:20,  3.65s/it][32m [repeated 5x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 17/37 [01:04<01:09,  3.48s/it][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=727504)[0m Loading checkpoint shards:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 18/37 [01:10<01:10,  3.69s/it][32m [repeated 5x across cluster][0m
[36m(WorkerDict pid=727504)[0m Loading checkpoint shards:  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 20/37 [01:17<01:02,  3.66s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 23/37 [01:24<00:46,  3.31s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25/37 [01:31<00:40,  3.41s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=727505)[0m Loading checkpoint shards:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 25/37 [01:36<00:43,  3.66s/it][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 28/37 [01:42<00:32,  3.66s/it][32m [repeated 5x across cluster][0m
[36m(WorkerDict pid=727505)[0m Loading checkpoint shards:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 28/37 [01:47<00:32,  3.65s/it][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=727504)[0m Loading checkpoint shards:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 30/37 [01:53<00:25,  3.65s/it][32m [repeated 5x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 32/37 [01:59<00:19,  3.83s/it][32m [repeated 7x across cluster][0m
[36m(WorkerDict pid=727504)[0m Loading checkpoint shards:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 33/37 [02:05<00:15,  3.80s/it][32m [repeated 5x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 34/37 [02:06<00:10,  3.66s/it]
[36m(WorkerDict pid=727505)[0m Loading checkpoint shards:  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 33/37 [02:06<00:15,  3.87s/it][32m [repeated 2x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 36/37 [02:12<00:03,  3.54s/it][32m [repeated 5x across cluster][0m
[36m(WorkerDict pid=727296)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [02:15<00:00,  3.39s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [02:15<00:00,  3.67s/it]
[36m(WorkerDict pid=727505)[0m Loading checkpoint shards:  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 36/37 [02:18<00:03,  3.96s/it][32m [repeated 6x across cluster][0m
[36m(WorkerDict pid=727505)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [02:22<00:00,  3.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 37/37 [02:22<00:00,  3.84s/it][32m [repeated 3x across cluster][0m
Error executing job with overrides: ['trainer.project_name=frozen_lake', 'system.CUDA_VISIBLE_DEVICES="0,1,2,3"', 'trainer.experiment_name=QWEN_72B-4GPU-ppo-basic-small-LoRA-1-4-all-batch_size_mini', 'algorithm.adv_estimator=gae', 'algorithm.kl_ctrl.kl_coef=0.001', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.clip_ratio_high=0.2', 'actor_rollout_ref.rollout.rollout_filter_ratio=1', 'actor_rollout_ref.rollout.max_model_len=2048', 'actor_rollout_ref.rollout.response_length=128', 'lora.rank=1', 'lora.alpha=4', 'lora.target_modules=all-linear', 'micro_batch_size_per_gpu=1', 'ppo_mini_batch_size=4', 'model_path=Qwen/Qwen2.5-72B-Instruct', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.75', 'trainer.n_gpus_per_node=4', 'actor_rollout_ref.rollout.tensor_model_parallel_size=4', 'actor_rollout_ref.rollout.engine_kwargs.swap_space=250', 'ray_init.num_cpus=64']
Traceback (most recent call last):
  File "/scr/jwiseman/RAGEN/train.py", line 302, in <module>
    main()
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
        ^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/train.py", line 158, in main
    run_ppo(config)
  File "/scr/jwiseman/RAGEN/train.py", line 179, in run_ppo
    ray.get(runner.run.remote(config))
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/worker.py", line 2822, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/worker.py", line 930, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::TaskRunner.run()[39m (pid=726762, ip=10.255.0.11, actor_id=b63439d9528c1628f05fdecb01000000, repr=<train.TaskRunner object at 0x7f80ad34ab40>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/train.py", line 296, in run
    trainer.init_workers()
  File "/scr/jwiseman/RAGEN/ragen/trainer/agent_trainer.py", line 360, in init_workers
    self.critic_wg.init_model()
  File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 49, in func
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.critic_init_model()[39m (pid=727503, ip=10.255.0.11, actor_id=25a99644e06399e17a69340001000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f263a44a060>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 459, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/single_controller/base/decorator.py", line 465, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 1044, in init_model
    self.critic_module, self.critic_optimizer, self.critic_lr_scheduler = self._build_critic_model_optimizer(self.config)
                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 974, in _build_critic_model_optimizer
    critic_module = FSDP(
                    ^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
    _init_param_handle_from_module(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 589, in _init_param_handle_from_module
    _materialize_with_param_init_fn(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 898, in _materialize_with_param_init_fn
    param_init_fn(module)
  File "/scr/jwiseman/RAGEN/verl/verl/utils/fsdp_utils.py", line 43, in init_fn
    x = x.to_empty(device=torch.cuda.current_device(), recurse=False)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1195, in to_empty
    return self._apply(
           ^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1196, in <lambda>
    lambda t: torch.empty_like(t, device=device), recurse=recurse
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/_prims_common/wrappers.py", line 291, in _fn
    result = fn(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/_refs/__init__.py", line 5003, in empty_like
    return torch.empty_permuted(
           ^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 924.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 345.69 MiB is free. Including non-PyTorch memory, this process has 44.17 GiB memory in use. Of the allocated memory 43.46 GiB is allocated by PyTorch, and 1.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(TaskRunner pid=726762)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.critic_init_model()[39m (pid=727505, ip=10.255.0.11, actor_id=4d3bada394692459b980aa2601000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f00a3521e80>)
[36m(TaskRunner pid=726762)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 459, in func
[36m(TaskRunner pid=726762)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=726762)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/scr/jwiseman/RAGEN/verl/verl/single_controller/base/decorator.py", line 465, in inner
[36m(TaskRunner pid=726762)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=726762)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 1044, in init_model
[36m(TaskRunner pid=726762)[0m     self.critic_module, self.critic_optimizer, self.critic_lr_scheduler = self._build_critic_model_optimizer(self.config)
[36m(TaskRunner pid=726762)[0m                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 974, in _build_critic_model_optimizer
[36m(TaskRunner pid=726762)[0m     critic_module = FSDP(
[36m(TaskRunner pid=726762)[0m                     ^^^^^
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[36m(TaskRunner pid=726762)[0m     _init_param_handle_from_module(
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 589, in _init_param_handle_from_module
[36m(TaskRunner pid=726762)[0m     _materialize_with_param_init_fn(
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 898, in _materialize_with_param_init_fn
[36m(TaskRunner pid=726762)[0m     param_init_fn(module)
[36m(TaskRunner pid=726762)[0m   File "/scr/jwiseman/RAGEN/verl/verl/utils/fsdp_utils.py", line 43, in init_fn
[36m(TaskRunner pid=726762)[0m     x = x.to_empty(device=torch.cuda.current_device(), recurse=False)
[36m(TaskRunner pid=726762)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1195, in to_empty
[36m(TaskRunner pid=726762)[0m     return self._apply(
[36m(TaskRunner pid=726762)[0m            ^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
[36m(TaskRunner pid=726762)[0m     param_applied = fn(param)
[36m(TaskRunner pid=726762)[0m                     ^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1196, in <lambda>
[36m(TaskRunner pid=726762)[0m     lambda t: torch.empty_like(t, device=device), recurse=recurse
[36m(TaskRunner pid=726762)[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/_prims_common/wrappers.py", line 291, in _fn
[36m(TaskRunner pid=726762)[0m     result = fn(*args, **kwargs)
[36m(TaskRunner pid=726762)[0m              ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/_refs/__init__.py", line 5003, in empty_like
[36m(TaskRunner pid=726762)[0m     return torch.empty_permuted(
[36m(TaskRunner pid=726762)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 924.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 385.69 MiB is free. Including non-PyTorch memory, this process has 44.13 GiB memory in use. Of the allocated memory 43.46 GiB is allocated by PyTorch, and 1.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(TaskRunner pid=726762)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.critic_init_model()[39m (pid=727504, ip=10.255.0.11, actor_id=e40a61b1e9e3f6a4798d97e601000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f981b11e0c0>)
[36m(TaskRunner pid=726762)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 459, in func
[36m(TaskRunner pid=726762)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=726762)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/scr/jwiseman/RAGEN/verl/verl/single_controller/base/decorator.py", line 465, in inner
[36m(TaskRunner pid=726762)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=726762)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 1044, in init_model
[36m(TaskRunner pid=726762)[0m     self.critic_module, self.critic_optimizer, self.critic_lr_scheduler = self._build_critic_model_optimizer(self.config)
[36m(TaskRunner pid=726762)[0m                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 974, in _build_critic_model_optimizer
[36m(TaskRunner pid=726762)[0m     critic_module = FSDP(
[36m(TaskRunner pid=726762)[0m                     ^^^^^
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[36m(TaskRunner pid=726762)[0m     _init_param_handle_from_module(
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 589, in _init_param_handle_from_module
[36m(TaskRunner pid=726762)[0m     _materialize_with_param_init_fn(
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 898, in _materialize_with_param_init_fn
[36m(TaskRunner pid=726762)[0m     param_init_fn(module)
[36m(TaskRunner pid=726762)[0m   File "/scr/jwiseman/RAGEN/verl/verl/utils/fsdp_utils.py", line 43, in init_fn
[36m(TaskRunner pid=726762)[0m     x = x.to_empty(device=torch.cuda.current_device(), recurse=False)
[36m(TaskRunner pid=726762)[0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1195, in to_empty
[36m(TaskRunner pid=726762)[0m     return self._apply(
[36m(TaskRunner pid=726762)[0m            ^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
[36m(TaskRunner pid=726762)[0m     param_applied = fn(param)
[36m(TaskRunner pid=726762)[0m                     ^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1196, in <lambda>
[36m(TaskRunner pid=726762)[0m     lambda t: torch.empty_like(t, device=device), recurse=recurse
[36m(TaskRunner pid=726762)[0m               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/_prims_common/wrappers.py", line 291, in _fn
[36m(TaskRunner pid=726762)[0m     result = fn(*args, **kwargs)
[36m(TaskRunner pid=726762)[0m              ^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/_refs/__init__.py", line 5003, in empty_like
[36m(TaskRunner pid=726762)[0m     return torch.empty_permuted(
[36m(TaskRunner pid=726762)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=726762)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 924.00 MiB. GPU 0 has a total capacity of 44.52 GiB of which 345.69 MiB is free. Including non-PyTorch memory, this process has 44.17 GiB memory in use. Of the allocated memory 43.46 GiB is allocated by PyTorch, and 1.95 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
