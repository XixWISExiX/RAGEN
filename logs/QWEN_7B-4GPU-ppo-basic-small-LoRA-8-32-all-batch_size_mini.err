/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/defaults_list.py:251: UserWarning: In '_3_frozen_lake': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information
  warnings.warn(msg, UserWarning)
2025-06-19 12:15:06,118	INFO worker.py:1888 -- Started a local Ray instance.
[36m(TaskRunner pid=823698)[0m DeprecationWarning: `ray.state.available_resources_per_node` is a private attribute and access will be removed in a future Ray version.
[36m(TaskRunner pid=823698)[0m WARNING:2025-06-19 12:15:15,093:Waiting for register center actor 7OPKIF_register_center to be ready. Elapsed time: 0 seconds out of 300 seconds.
[36m(WorkerDict pid=824234)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`
[36m(WorkerDict pid=824234)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[36m(WorkerDict pid=824441)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]
[36m(WorkerDict pid=824234)[0m Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:03<00:10,  3.52s/it]
[36m(WorkerDict pid=824440)[0m Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in Qwen2ForCausalLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained("openai/whisper-tiny", attn_implementation="flash_attention_2", torch_dtype=torch.float16)`[32m [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)[0m
[36m(WorkerDict pid=824440)[0m You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.[32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=824440)[0m Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s][32m [repeated 3x across cluster][0m
[36m(WorkerDict pid=824234)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:10<00:03,  3.34s/it][32m [repeated 8x across cluster][0m
[36m(WorkerDict pid=824234)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.20s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:13<00:00,  3.27s/it]
[36m(WorkerDict pid=824440)[0m Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:11<00:03,  3.85s/it][32m [repeated 3x across cluster][0m
Error executing job with overrides: ['trainer.project_name=frozen_lake', 'system.CUDA_VISIBLE_DEVICES="0,1,2,3"', 'trainer.experiment_name=QWEN_7B-4GPU-ppo-basic-small-LoRA-8-32-all-batch_size_mini', 'algorithm.adv_estimator=gae', 'algorithm.kl_ctrl.kl_coef=0.001', 'actor_rollout_ref.actor.kl_loss_coef=0.001', 'actor_rollout_ref.actor.clip_ratio_high=0.2', 'actor_rollout_ref.rollout.rollout_filter_ratio=1', 'actor_rollout_ref.rollout.max_model_len=2048', 'actor_rollout_ref.rollout.response_length=128', 'lora.rank=8', 'lora.alpha=32', 'lora.target_modules=all-linear', 'micro_batch_size_per_gpu=1', 'ppo_mini_batch_size=4', 'model_path=Qwen/Qwen2.5-7B-Instruct', 'actor_rollout_ref.rollout.gpu_memory_utilization=0.75', 'trainer.n_gpus_per_node=4', 'actor_rollout_ref.rollout.tensor_model_parallel_size=4']
Traceback (most recent call last):
  File "/scr/jwiseman/RAGEN/train.py", line 302, in <module>
    main()
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/main.py", line 94, in decorated_main
    _run_hydra(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 394, in _run_hydra
    _run_app(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 457, in _run_app
    run_and_report(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 223, in run_and_report
    raise ex
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 220, in run_and_report
    return func()
           ^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/utils.py", line 458, in <lambda>
    lambda: hydra.run(
            ^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/_internal/hydra.py", line 132, in run
    _ = ret.return_value
        ^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/core/utils.py", line 260, in return_value
    raise self._return_value
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/hydra/core/utils.py", line 186, in run_job
    ret.return_value = task_function(task_cfg)
                       ^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/train.py", line 158, in main
    run_ppo(config)
  File "/scr/jwiseman/RAGEN/train.py", line 179, in run_ppo
    ray.get(runner.run.remote(config))
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/worker.py", line 2822, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/ray/_private/worker.py", line 930, in get_objects
    raise value.as_instanceof_cause()
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::TaskRunner.run()[39m (pid=823698, ip=10.255.0.11, actor_id=6faac1ea46b830e7ab45ee8c01000000, repr=<train.TaskRunner object at 0x7f762cee6c00>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/train.py", line 296, in run
    trainer.init_workers()
  File "/scr/jwiseman/RAGEN/ragen/trainer/agent_trainer.py", line 360, in init_workers
    self.critic_wg.init_model()
  File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 49, in func
    output = ray.get(output)
             ^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ray.exceptions.RayTaskError(OutOfMemoryError): [36mray::WorkerDict.critic_init_model()[39m (pid=824234, ip=10.255.0.11, actor_id=e1298d287dcd5a21d832e45101000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7f5324f7a480>)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 459, in func
    return getattr(self.worker_dict[key], name)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/verl/verl/single_controller/base/decorator.py", line 465, in inner
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 1044, in init_model
    self.critic_module, self.critic_optimizer, self.critic_lr_scheduler = self._build_critic_model_optimizer(self.config)
                                                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 974, in _build_critic_model_optimizer
    critic_module = FSDP(
                    ^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
    _init_param_handle_from_module(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 636, in _init_param_handle_from_module
    _init_param_handle_from_params(state, managed_params, fully_sharded_module)
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 648, in _init_param_handle_from_params
    handle = FlatParamHandle(
             ^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 602, in __init__
    self._init_flat_param_and_metadata(
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 761, in _init_flat_param_and_metadata
    self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(
                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 883, in flatten_tensors_into_flat_param
    flat_param_data = self.flatten_tensors(tensors, aligned_numel)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 875, in flatten_tensors
    return torch.cat(flat_tensors, dim=0)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.37 GiB. GPU 0 has a total capacity of 44.52 GiB of which 15.11 GiB is free. Including non-PyTorch memory, this process has 29.40 GiB memory in use. Of the allocated memory 28.53 GiB is allocated by PyTorch, and 169.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(TaskRunner pid=823698)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.critic_init_model()[39m (pid=824441, ip=10.255.0.11, actor_id=9ed789354a769a1400a8c43c01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fbe9ab820c0>)
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 459, in func
[36m(TaskRunner pid=823698)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/scr/jwiseman/RAGEN/verl/verl/single_controller/base/decorator.py", line 465, in inner
[36m(TaskRunner pid=823698)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 1044, in init_model
[36m(TaskRunner pid=823698)[0m     self.critic_module, self.critic_optimizer, self.critic_lr_scheduler = self._build_critic_model_optimizer(self.config)
[36m(TaskRunner pid=823698)[0m                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 974, in _build_critic_model_optimizer
[36m(TaskRunner pid=823698)[0m     critic_module = FSDP(
[36m(TaskRunner pid=823698)[0m                     ^^^^^
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[36m(TaskRunner pid=823698)[0m     _init_param_handle_from_module(
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 636, in _init_param_handle_from_module
[36m(TaskRunner pid=823698)[0m     _init_param_handle_from_params(state, managed_params, fully_sharded_module)
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 648, in _init_param_handle_from_params
[36m(TaskRunner pid=823698)[0m     handle = FlatParamHandle(
[36m(TaskRunner pid=823698)[0m              ^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 602, in __init__
[36m(TaskRunner pid=823698)[0m     self._init_flat_param_and_metadata(
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 761, in _init_flat_param_and_metadata
[36m(TaskRunner pid=823698)[0m     self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(
[36m(TaskRunner pid=823698)[0m                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 883, in flatten_tensors_into_flat_param
[36m(TaskRunner pid=823698)[0m     flat_param_data = self.flatten_tensors(tensors, aligned_numel)
[36m(TaskRunner pid=823698)[0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 875, in flatten_tensors
[36m(TaskRunner pid=823698)[0m     return torch.cat(flat_tensors, dim=0)
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.37 GiB. GPU 0 has a total capacity of 44.52 GiB of which 15.14 GiB is free. Including non-PyTorch memory, this process has 29.37 GiB memory in use. Of the allocated memory 28.53 GiB is allocated by PyTorch, and 169.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(TaskRunner pid=823698)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.critic_init_model()[39m (pid=824440, ip=10.255.0.11, actor_id=eff84120491c228be1e973e901000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fd96b90a090>)
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 459, in func
[36m(TaskRunner pid=823698)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/scr/jwiseman/RAGEN/verl/verl/single_controller/base/decorator.py", line 465, in inner
[36m(TaskRunner pid=823698)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 1044, in init_model
[36m(TaskRunner pid=823698)[0m     self.critic_module, self.critic_optimizer, self.critic_lr_scheduler = self._build_critic_model_optimizer(self.config)
[36m(TaskRunner pid=823698)[0m                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 974, in _build_critic_model_optimizer
[36m(TaskRunner pid=823698)[0m     critic_module = FSDP(
[36m(TaskRunner pid=823698)[0m                     ^^^^^
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[36m(TaskRunner pid=823698)[0m     _init_param_handle_from_module(
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 636, in _init_param_handle_from_module
[36m(TaskRunner pid=823698)[0m     _init_param_handle_from_params(state, managed_params, fully_sharded_module)
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 648, in _init_param_handle_from_params
[36m(TaskRunner pid=823698)[0m     handle = FlatParamHandle(
[36m(TaskRunner pid=823698)[0m              ^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 602, in __init__
[36m(TaskRunner pid=823698)[0m     self._init_flat_param_and_metadata(
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 761, in _init_flat_param_and_metadata
[36m(TaskRunner pid=823698)[0m     self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(
[36m(TaskRunner pid=823698)[0m                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 883, in flatten_tensors_into_flat_param
[36m(TaskRunner pid=823698)[0m     flat_param_data = self.flatten_tensors(tensors, aligned_numel)
[36m(TaskRunner pid=823698)[0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 875, in flatten_tensors
[36m(TaskRunner pid=823698)[0m     return torch.cat(flat_tensors, dim=0)
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.37 GiB. GPU 0 has a total capacity of 44.52 GiB of which 15.11 GiB is free. Including non-PyTorch memory, this process has 29.41 GiB memory in use. Of the allocated memory 28.53 GiB is allocated by PyTorch, and 169.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(TaskRunner pid=823698)[0m Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): [36mray::WorkerDict.critic_init_model()[39m (pid=824439, ip=10.255.0.11, actor_id=0d2146e2413d01384170e6eb01000000, repr=<verl.single_controller.ray.base.WorkerDict object at 0x7fa49409dd00>)
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/scr/jwiseman/RAGEN/verl/verl/single_controller/ray/base.py", line 459, in func
[36m(TaskRunner pid=823698)[0m     return getattr(self.worker_dict[key], name)(*args, **kwargs)
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/scr/jwiseman/RAGEN/verl/verl/single_controller/base/decorator.py", line 465, in inner
[36m(TaskRunner pid=823698)[0m     return func(*args, **kwargs)
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 1044, in init_model
[36m(TaskRunner pid=823698)[0m     self.critic_module, self.critic_optimizer, self.critic_lr_scheduler = self._build_critic_model_optimizer(self.config)
[36m(TaskRunner pid=823698)[0m                                                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/scr/jwiseman/RAGEN/ragen/workers/fsdp_workers.py", line 974, in _build_critic_model_optimizer
[36m(TaskRunner pid=823698)[0m     critic_module = FSDP(
[36m(TaskRunner pid=823698)[0m                     ^^^^^
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/fully_sharded_data_parallel.py", line 509, in __init__
[36m(TaskRunner pid=823698)[0m     _init_param_handle_from_module(
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 636, in _init_param_handle_from_module
[36m(TaskRunner pid=823698)[0m     _init_param_handle_from_params(state, managed_params, fully_sharded_module)
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_init_utils.py", line 648, in _init_param_handle_from_params
[36m(TaskRunner pid=823698)[0m     handle = FlatParamHandle(
[36m(TaskRunner pid=823698)[0m              ^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 602, in __init__
[36m(TaskRunner pid=823698)[0m     self._init_flat_param_and_metadata(
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 761, in _init_flat_param_and_metadata
[36m(TaskRunner pid=823698)[0m     self.flat_param: FlatParameter = self.flatten_tensors_into_flat_param(
[36m(TaskRunner pid=823698)[0m                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 883, in flatten_tensors_into_flat_param
[36m(TaskRunner pid=823698)[0m     flat_param_data = self.flatten_tensors(tensors, aligned_numel)
[36m(TaskRunner pid=823698)[0m                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m   File "/home/student/jwiseman/.conda/envs/ragen/lib/python3.12/site-packages/torch/distributed/fsdp/_flat_param.py", line 875, in flatten_tensors
[36m(TaskRunner pid=823698)[0m     return torch.cat(flat_tensors, dim=0)
[36m(TaskRunner pid=823698)[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[36m(TaskRunner pid=823698)[0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 28.37 GiB. GPU 0 has a total capacity of 44.52 GiB of which 15.11 GiB is free. Including non-PyTorch memory, this process has 29.41 GiB memory in use. Of the allocated memory 28.53 GiB is allocated by PyTorch, and 169.47 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[36m(WorkerDict pid=824440)[0m Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.69s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:15<00:00,  3.78s/it][32m [repeated 3x across cluster][0m
