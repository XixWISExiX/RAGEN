defaults: # Call this config file before running the rest of this config file.
  - ppo_trainer # this is a symbolic link to the verl/verl/trainer/config/ppo_trainer.yaml file
  - envs

system:
  CUDA_VISIBLE_DEVICES: "0" # Forces the trainer process to see only GPU 0 by default.

micro_batch_size_per_gpu: 4 # Number of prompts generated on each GPU before PPO update.
ppo_mini_batch_size: 32   # trajectories *per optimizer step*
# Example: n_gpus = 2, micro_batch_size_per_gpu = 4
#          => total batch B = 2 × 4 = 8 trajectories
#          ppo_mini_batch_size = 2
#          => k = ⌈8 / 2⌉ = 4 optimizer updates this epoch
#
# NOTE: More optimizer updates (smaller mini-batch) slow wall-time
#       but reduce GPU-memory needed *during the update*
# ALSO: The higher the mini_batch_size the larger and the stronger the update is, so if the example had a ppo_mini_batch_size = 16, it would 2x the trajactory count with sampling and update the gradient.
model_path: Qwen/Qwen2.5-0.5B-Instruct # Model to load from HuggingFace 
enable_response_mask: True # Masks out the prompt portion so generated responses are the ones handled computationally. # Enabling response mask could improve stability of rollout/old_log_prob, as P(st|history) are no longer calculated in loss here. See https://docs.google.com/document/d/1bg7obeiKTExuHHBl5uOiSpec5uLDZ2Tgvxy6li5pHX4/edit?usp=sharing for more details.
grpo_advantage_length_weight: False # When using GRPO, when set this multiplies the advantage by the response-length which rewards longer reasoning chains. # if you do not enable this and critic/advantage_estimator is GRPO, and the critic/advantages/mean is too low, then you can try enabling this to encourage reasoning and forbid collapse

lora: # Efficient Fine-tunning
  rank: 0 # If rank=0 enable full-fine-tunning, if rank >= 1 enable rank adapters of size (in,r) & (r, out). When r is small, updates have a less capacity. NOTE: use 8-64 for small models to save VRAM.
  alpha: 16 # Alpha in the lora equation delta w = alpha / r * BA. A higher alpha lets small ranks act like larger ones. A higher alpha = stronger update, but also riskier instability. NOTE: It's common to set alpha = r or alpha = 2*r.
  target_modules: all-linear # Which layers get lora adapters. In this case, it's all the linear layers. NOTE: This requires more VRAM than just changing the attention layers, but still cheaper VRAM wise than full fine-tuning.

actor_rollout_ref:
  model:
    path: ${model_path}
    lora_rank: ${lora.rank}
    lora_alpha: ${lora.alpha}
    target_modules: ${lora.target_modules}
  actor:
    ppo_mini_batch_size: ${ppo_mini_batch_size}  # by default, ppo_mini_batch_size = train_batch_size / 4
    micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # following micro_batch_size_per_gpu
    ppo_micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # following micro_batch_size_per_gpu
    use_ref: True # Actor loss uses ref-model KL penalty (Needs to be true of using KL-regularized PPO)
    entropy_coeff: 0.001 # The higher the coef, the more entropy (less discreteness) there is.
    use_kl_loss: False # KL loss function utilization
    kl_loss_coef: 0.000 # The higher the coef, the higher the model is punished for deviating.
    kl_loss_type: kl 
    clip_ratio_low: 0.2 # 0.2 deviation allowed on the lower end.
    clip_ratio_high: 0.28 # 0.28 deviation allowed on the higher end.
    grpo_advantage_length_weight: ${grpo_advantage_length_weight} # GRPO boolean condition
    optim:
      betas: [0.9, 0.999] # Standard Adam betas, which are exponential decay rates for the statistical moments. NOTE: The higher the beta, the less responsive the model will be to short-term events.
  ref:
    log_prob_micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # following micro_batch_size_per_gpu
  rollout:
    log_prob_micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # following micro_batch_size_per_gpu
    tensor_model_parallel_size: 1 # Model fits on n_gpus
    max_model_len: 3600 # Upper bound on prompt+response tokens for generation.
    prompt_length: 1 # useless. Just put it here DUMMY_VALUE!
    response_length: 400 # single-turn response length (Cap on tokens generated per turn)
    gpu_memory_utilization: 0.5 # Target CUDA memory fraction for DeepSpeed's Auto-batcher.
    max_num_batched_tokens: 8192 # set only when enable_chunked_prefill is true. Ceiling for chunked pre-fill batching.
    temperature: 1 # Randomness factor when generating rollouts (the lower, the more deterministic).
    rollout_filter_ratio: 0.25 # Discard top % of outlier rewards based of filters.
    rollout_filter_type: std # max_mean or std filter.
    enforce_eager: True # Eager Tensors May help with debugging, but are slower. # for small models, set both enforce_eager and free_cache_engine to False to make rollout faster
    free_cache_engine: True # Free KV-cache between turns
    val_kwargs:
      do_sample: True
      temperature: 0.5 # Lower sampling temp during validation to stabilise metrics.
    tp_size_check: true # Check that TP (Tensor Parallelism) size matches the number of GPUs

# Metrics her are similar to actor
critic:
  ppo_mini_batch_size: ${ppo_mini_batch_size} # by default, ppo_mini_batch_size = train_batch_size / 4
  ppo_micro_batch_size_per_gpu: ${micro_batch_size_per_gpu} # following micro_batch_size_per_gpu
  model:
    path: ${model_path}
    lora_rank: ${lora.rank}
    lora_alpha: ${lora.alpha}
    target_modules: ${lora.target_modules}
  optim:
    betas: [0.9, 0.999]

# Probably dynamically computed values at runtime.
data:
  max_prompt_length: null
  max_response_length: null
  train_batch_size: null

algorithm:
  gamma: 1.0 # Discount factor. NOTE: 1 here meas all future rewards matter equally.
  lam: 1.0 # GAE lambda, a higher value means smoother longer advantage estimate. NOTE: here that means the advantage estimate is monte-carlo level.
  high_level_gamma: 0.95 # Discount factor for the high-level policy (used for HRL or Hierarchical-RL).
  adv_estimator: gae # the advantage estimator used (A)
  bi_level_gae: False # In bi-level GAE, you compute two separate sets of advantages, one for high-level actions, one for low-level actions. NOTE: This cal also allow you to apply different reward signals to different stages.
  kl_penalty: kl # Indicates what KL pentalty to compute # how to estimate kl divergence
  kl_ctrl: # Controls the kl
    type: fixed # fixed/adaptive/disabled kl_coef types
    kl_coef: 0.000 # Coef to how strong the kl pentalty is.

trainer:
  project_name: ragen_latest # wandb project name
  experiment_name: test # wandb experiment name
  total_training_steps: 200 # total number of global PPO update steps
  validation_steps: 1 # The number of evaluation batches per validation pass. # validation instances = validation_steps * val_env_groups * group_size
  val_before_train: True # Run validation before training steps to get baseline metrics.
  n_gpus_per_node: 1 # Number of visible GPUs for the trainer process.
  test_freq: 10 # Do validation every 10 training steps.
  generations_to_log_to_wandb: # Max number of prompt-generation rows stored in wandb tables.
    train: 128 # TODO: will be implemented
    val: 20
  logger: [ 'console', 'wandb' ] # Places to log the results from the trainer.

agent_proxy:
  max_turn: 5 # Episodes can run at most 5 dialogue turns.
  action_sep: "||" # The action separator symbol.
  max_actions_per_turn: 5 # how many actions can be output at most in a single turn
  use_turn_scores: False # Setting this to False makes Monte-Carlo like reward schemes. Setting this to True will make the tokens have their own reward baseline. # important to GAE when applying token-level rewards to token-level advantages. If False, will take the sum of scores as the reward for the last turn.
  enable_think: True # Enables the reasoning tokens. # False -> no think RL
  reward_normalization:
    grouping: "state" # state (token-level) / batch (whole roll-out batch) / inductive (global states across training steps) # computes mean/std
    method: "identity" # asym_clip / identity (nothing) / mean_std # shapes the reward

es_manager: # Environment-Suite Manager
  format_penalty: -0.1 # Negative reward applied when the LLM's output violates the required answer format.
  train:
    env_groups: 8 # How many parallel groups run on the node (unique seeds). NOTE: no VRAM overhead
    # under the same group, the env config and env seed are ensured to be equal
    group_size: 16 # How many identical env instances live inside one group (same seeds), this is a sampling trick for faster compute, but less diversity. NOTE: more group_size = more VRAM
    # Total envs to sample is now env_groups * group_size
    env_configs:
      tags: ["SimpleSokoban"] # Environment to pull from gym-style.
      n_groups: [8] # A tensor batch of prompts that all go through the model in one forward pass. # If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation
  val:
    env_groups: 256
    group_size: 1 # should be set to 1 because when val temperature is set to 0 and group size > 1, there will be repetitive prompts which leads to same trajectory.
    env_configs:
      tags: ["SimpleSokoban"]
      n_groups: [256] # TODO: If not set, all env names divide nums equally. Under the same group, the env config and env seed (prompt) are equal in each generation

ctx_manager: # Governs how the actor actually samples tokens during rollouts.
  generation: # go to vllm
    gen_config:
      response_length: ${actor_rollout_ref.rollout.response_length}
      temperature: ${actor_rollout_ref.rollout.temperature}
      top_p: ${actor_rollout_ref.rollout.top_p}
      top_k: ${actor_rollout_ref.rollout.top_k}
      kwargs: null
